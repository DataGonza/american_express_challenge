{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd07d3bf-20a5-4def-965a-ecc633fc939b",
   "metadata": {},
   "source": [
    "# Preparating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eaf783d-eb9e-4065-9e31-db4d43f2acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e246f04b-6a1d-4670-983f-7231a0b33234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import variables of all process\n",
    "sample_frac = 0.25\n",
    "frac_nan_values = 0.05\n",
    "\n",
    "#some important dicts\n",
    "map_D63 = dict(zip(['CR', 'CO', 'CL', 'XL', 'XZ', 'XM'], range(6)))\n",
    "map_D64 = dict(zip(['O', 'R', np.nan, 'U', '-1'], [0, 1, np.nan, 2, 3]))\n",
    "columns_dtype = {'B_31' : 'int8', **json.load(open('columns_map.json'))}\n",
    "columns_to_drop = ['S_3', 'D_42', 'D_43', 'D_46', 'D_48', 'D_49', 'D_50', 'D_53',\n",
    "                   'S_7', 'D_56', 'S_9', 'D_62', 'B_17', 'D_66', 'D_73', 'D_76',\n",
    "                   'D_77', 'R_9', 'D_82', 'B_29', 'D_87', 'D_88', 'D_105', 'D_106',\n",
    "                   'R_26', 'R_27', 'D_108', 'D_110', 'D_111', 'B_39', 'S_27', 'B_42',\n",
    "                   'D_132', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'D_142','S_2']\n",
    "\n",
    "\n",
    "# Reading data in chunks\n",
    "all_columns = list(pd.read_csv('C:/Users/DavidG/Documents/american_express_data/train_data.csv', nrows =1))\n",
    "columns_to_read = np.setdiff1d(all_columns, columns_to_drop)\n",
    "train_result = pd.read_csv('C:/Users/DavidG/Documents/american_express_data/train_labels.csv', dtype = {'target': 'int8'}) # Read the result of train data\n",
    "\n",
    "train_partitions = np.array_split(train_result.sample(frac = 1, random_state = 12345), int(1/sample_frac))\n",
    "\n",
    "\n",
    "#print(\"Fraction of 1 in the complete dataset is {}\".format(sum(train_result['target'])/len(train_result))) # Showing the fraction of 1 in the all data\n",
    "#print(\"Fraction of 1 in the sample dataset is {}\".format(sum(train_result_sample['target'])/len(train_result_sample))) # Showing the fraction of 1 in the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d058e693-87e8-4a97-ba8b-52a1c4d683cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready the lecture of 0 partition\n",
      "Ready apply of 0 partition\n",
      "Ready add n values of 0 partition\n",
      "Ready m_values of 0 partition\n",
      "Ready other features of 0 partition and save\n",
      "Ready the lecture of 1 partition\n",
      "Ready apply of 1 partition\n",
      "Ready add n values of 1 partition\n",
      "Ready m_values of 1 partition\n",
      "Ready other features of 1 partition and save\n",
      "Ready the lecture of 2 partition\n",
      "Ready apply of 2 partition\n",
      "Ready add n values of 2 partition\n",
      "Ready m_values of 2 partition\n",
      "Ready other features of 2 partition and save\n",
      "Ready the lecture of 3 partition\n",
      "Ready apply of 3 partition\n",
      "Ready add n values of 3 partition\n",
      "Ready m_values of 3 partition\n",
      "Ready other features of 3 partition and save\n"
     ]
    }
   ],
   "source": [
    "n = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])\n",
    "div_val = n*((n*n).cumsum()) - (n.cumsum())**2\n",
    "div = dict(zip(n, div_val))\n",
    "div = {0:np.nan, **div}\n",
    "div[1] = np.nan\n",
    "sum_val = dict(zip(n, n.cumsum()))\n",
    "sum_val = {0:np.nan, **sum_val}\n",
    "\n",
    "for k, partition in enumerate(train_partitions):\n",
    "    train_data = pd.read_csv('C:/Users/DavidG/Documents/american_express_data/train_data.csv', chunksize = 500000, dtype = columns_dtype, usecols = columns_to_read)\n",
    "    train_result_sample = partition\n",
    "    chunk_list = []\n",
    "    for chunk in train_data:\n",
    "        chunk_list.append(pd.merge(train_result_sample, chunk, how = 'inner', on = 'customer_ID'))\n",
    "\n",
    "    train_data_sample = pd.concat(chunk_list)\n",
    "\n",
    "\n",
    "    del chunk_list\n",
    "    print('Ready the lecture of {} partition'.format(k))\n",
    "    \n",
    "    \n",
    "    train_data_sample['D_63'] = train_data_sample.apply(lambda x: map_D63[x['D_63']], axis = 1)\n",
    "    train_data_sample['D_64'] = train_data_sample.apply(lambda x: map_D64[x['D_64']], axis = 1)\n",
    "    print('Ready apply of {} partition'.format(k))\n",
    "    \n",
    "    train_data_sample.reset_index(drop = True, inplace = True)\n",
    "    train_data_sample['n'] = np.ones(len(train_data_sample), dtype = 'int8')\n",
    "    j = 1\n",
    "    for i in range(1, len(train_data_sample)):\n",
    "        if train_data_sample.loc[(i-1, 'customer_ID')] == train_data_sample.loc[(i, 'customer_ID')]:\n",
    "            j += 1\n",
    "        else:\n",
    "            j = 1\n",
    "        train_data_sample.loc[(i, 'n')] = j\n",
    "    cont_pd = train_data_sample.drop(['target', 'n'], axis = 1).groupby('customer_ID').count()\n",
    "    print('Ready add n values of {} partition'.format(k))\n",
    "    \n",
    "    # m values\n",
    "    m_values = train_data_sample.drop(['target', 'n', 'customer_ID'], axis = 1).multiply(train_data_sample['n'], axis = 0)\n",
    "    m_values = m_values.astype('float32')\n",
    "    m_values = pd.concat([m_values, train_data_sample['customer_ID']], axis = 1)\n",
    "    m_values = m_values.groupby('customer_ID').sum() * cont_pd\n",
    "    m_values = m_values + cont_pd.applymap(lambda x: -sum_val[x])*train_data_sample.drop(['target', 'n'], axis = 1).groupby('customer_ID').prod()\n",
    "    m_values = m_values.astype('float32')\n",
    "    m_values = m_values / cont_pd.applymap(lambda x: div[x])\n",
    "    m_values = m_values.astype('float32')\n",
    "    print('Ready m_values of {} partition'.format(k))\n",
    "    \n",
    "    # Other features\n",
    "    X_train = train_data_sample.drop(['target'], axis = 1).groupby('customer_ID').agg([np.mean, np.max, np.min, np.std])\n",
    "    X_train = pd.DataFrame(data = X_train.values.astype('float32'), index = X_train.index, columns = [x[0] + '_' + x[1] for x in X_train.columns])\n",
    "    X_train = pd.merge(X_train, m_values, how = 'inner', left_index = True, right_index = True)\n",
    "    final_data_dropna = pd.merge(X_train.dropna(), train_result_sample, on = 'customer_ID', how = 'inner')\n",
    "    final_data_fillna = pd.merge(X_train.fillna(X_train.mean()), train_result_sample, on = 'customer_ID', how = 'inner')\n",
    "    final_data_dropna.to_parquet('C:/Users/DavidG/Documents/american_express_data/dropna_data_{}.parquet'.format(k))\n",
    "    final_data_fillna.to_parquet('C:/Users/DavidG/Documents/american_express_data/fillna_data_{}.parquet'.format(k))\n",
    "    print('Ready other features of {} partition and save'.format(k))\n",
    "    \n",
    "    # Eliminate variables\n",
    "    del m_values\n",
    "    del X_train\n",
    "    del train_data_sample\n",
    "    del train_result_sample\n",
    "    del cont_pd\n",
    "    del final_data_dropna\n",
    "    del final_data_fillna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
